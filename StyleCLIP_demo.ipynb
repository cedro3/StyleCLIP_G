{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cedro3/StyleCLIP/blob/master/StyleCLIP_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjH5m1vlR10j"
   },
   "source": [
    "# セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be8q81qe0geb"
   },
   "outputs": [],
   "source": [
    "# githubからコードを取得\n",
    "! git clone https://github.com/cedro3/StyleCLIP_G.git\n",
    "%cd StyleCLIP_G\n",
    "\n",
    "# Pytorch バージョン変更\n",
    "! pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html \n",
    "\n",
    "# CLIP インストール\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pipi install ftfy regex\n",
    "\n",
    "# 学習済みパラメータのダウンロード\n",
    "!wget https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BBtjlks6-yN"
   },
   "source": [
    "# テキストから顔画像を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnAw8uGANduc"
   },
   "outputs": [],
   "source": [
    "text = \"She is a charming woman with blonde hair and blue eyes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7efh-D9Joqou"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from stylegan_models import g_all, g_synthesis, g_mapping\n",
    "from utils import GetFeatureMaps, transform_img, compute_loss\n",
    "from tqdm import trange\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')   \n",
    "\n",
    "# picフォルダーリセット\n",
    "import os\n",
    "import shutil\n",
    "if os.path.isdir('pic'):\n",
    "     shutil.rmtree('pic')\n",
    "os.makedirs('pic', exist_ok=True)\n",
    "\n",
    "# 初期設定 \n",
    "lr = 1e-2 \n",
    "img_save_freq = 2\n",
    "max_iter = 101 \n",
    "ref_img_path = None \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"USING \", device)\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "vgg_layers = vgg16.features\n",
    "\n",
    "vgg_layer_name_mapping = {\n",
    "    '1': \"relu1_1\",\n",
    "    '3': \"relu1_2\",\n",
    "    '6': \"relu2_1\",\n",
    "    '8': \"relu2_2\",\n",
    "    # '15': \"relu3_3\",\n",
    "    # '22': \"relu4_3\"\n",
    "}\n",
    "\n",
    "g_synthesis.eval()\n",
    "g_synthesis.to(device)\n",
    "\n",
    "latent_shape = (1, 1, 512)\n",
    "\n",
    "normal_generator = torch.distributions.normal.Normal(\n",
    "    torch.tensor([0.0]),\n",
    "    torch.tensor([1.]),\n",
    ")\n",
    "\n",
    "# init_latents = normal_generator.sample(latent_shape).squeeze(-1).to(device)\n",
    "latents_init = torch.zeros(latent_shape).squeeze(-1).to(device)\n",
    "latents = torch.nn.Parameter(latents_init, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=[latents],\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "\n",
    "def truncation(x, threshold=0.7, max_layer=8):\n",
    "    avg_latent = torch.zeros(1, x.size(1), 512).to(device)\n",
    "    interp = torch.lerp(avg_latent, x, threshold)\n",
    "    do_trunc = (torch.arange(x.size(1)) < max_layer).view(1, -1, 1).to(device)\n",
    "    return torch.where(do_trunc, interp, x)\n",
    "\n",
    "def tensor_to_pil_img(img):\n",
    "    img = (img.clamp(-1, 1) + 1) / 2.0\n",
    "    img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 256\n",
    "    img = Image.fromarray(img.astype('uint8'))\n",
    "    return img\n",
    "\n",
    "\n",
    "clip_transform = torchvision.transforms.Compose([\n",
    "    # clip_preprocess.transforms[2],\n",
    "    clip_preprocess.transforms[4],\n",
    "])\n",
    "\n",
    "if ref_img_path is None:\n",
    "    ref_img = None\n",
    "else:\n",
    "    ref_img = clip_preprocess(Image.open(ref_img_path)).unsqueeze(0).to(device)\n",
    "\n",
    "clip_normalize = torchvision.transforms.Normalize(\n",
    "    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "    std=(0.26862954, 0.26130258, 0.27577711),\n",
    ")\n",
    "\n",
    "def compute_clip_loss(img, text):\n",
    "    # img = clip_transform(img)\n",
    "    img = torch.nn.functional.upsample_bilinear(img, (224, 224))\n",
    "    tokenized_text = clip.tokenize([text]).to(device)\n",
    "    img_logits, _text_logits = clip_model(img, tokenized_text)\n",
    "    return 1/img_logits * 100\n",
    "\n",
    "def compute_perceptual_loss(gen_img, ref_img):\n",
    "    gen_img = torch.nn.functional.upsample_bilinear(img, (224, 224))\n",
    "    loss = 0\n",
    "    len_vgg_layer_mappings = int(max(vgg_layer_name_mapping.keys()))\n",
    "    ref_feats = ref_img\n",
    "    gen_feats = gen_img\n",
    "\n",
    "    for idx, (name, module) in enumerate(vgg_layers._modules.items()):\n",
    "        ref_feats = module(ref_feats)\n",
    "        gen_feats = module(gen_feats)\n",
    "        if name in vgg_layer_name_mapping.keys():\n",
    "            loss += torch.nn.functional.mse_loss(ref_feats, gen_feats)\n",
    "        \n",
    "        if idx >= len_vgg_layer_mappings:\n",
    "            break\n",
    "    \n",
    "    return loss/len_vgg_layer_mappings\n",
    "\n",
    "counter = 0\n",
    "for i in trange(max_iter):\n",
    "    dlatents = latents.repeat(1,18,1)\n",
    "    img = g_synthesis(dlatents)\n",
    "    loss = compute_clip_loss(img, text) \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % img_save_freq == 0:\n",
    "        img = tensor_to_pil_img(img)\n",
    "        img = img.resize((512,512))  \n",
    "        img.save(os.path.join('./pic', str(counter).zfill(6)+'.png')) \n",
    "        counter +=1 \n",
    "\n",
    "# 最終画像を表示\n",
    "from IPython.display import Image, display_png\n",
    "display_png(Image('./pic/'+str(counter-1).zfill(6)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUnS-Df7H3_q"
   },
   "outputs": [],
   "source": [
    "# --- 静止画からmp4を作成 ---\n",
    "\n",
    "# 既に output.mp4 があれば削除する\n",
    "import os\n",
    "if os.path.exists('./output.mp4'):\n",
    "   os.remove('./output.mp4')\n",
    "\n",
    "# pic フォルダーの静止画から動画を作成\n",
    "! ffmpeg -r 15 -i pic/%6d.png\\\n",
    "               -vcodec libx264 -pix_fmt yuv420p output.mp4\n",
    "\n",
    "# movieフォルダへ名前を付けてコピー\n",
    "import shutil\n",
    "os.makedirs('movie', exist_ok=True)\n",
    "shutil.copy('output.mp4', 'movie/'+text+'.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA8C3uTH6gPd"
   },
   "outputs": [],
   "source": [
    "# --- mp4動画の再生 ---\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open('./output.mp4', 'rb').read()\n",
    "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "HTML(f\"\"\"\n",
    "<video width=\"50%\" height=\"50%\" controls>\n",
    "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPB1FZZHqg75O4JMqr5x0kH",
   "include_colab_link": true,
   "name": "StyleCLIP_demo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
